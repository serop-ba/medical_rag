{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 225 chunks\n"
     ]
    }
   ],
   "source": [
    "# Load web page\n",
    "import argparse\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embed and store\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings, OpenAIEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loader = PyPDFLoader('../..//data/raw/motor_neuron_disease.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "# Split into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(f\"Split into {len(all_splits)} chunks\")\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits[:5], embedding=OllamaEmbeddings(model='phi3'))\n",
    "\n",
    "# qdrant = QdrantVectorStore.from_documents(\n",
    "#     all_splits[:5],\n",
    "#     OllamaEmbeddings(model='phi3'),\n",
    "#     path=\"local_qdrant\",\n",
    "#     collection_name=\"my_documents\",\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mhi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:346\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    343\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    344\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 346\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    347\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    348\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    349\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    350\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    351\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    352\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    353\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    354\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m         \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    357\u001b[0m         \u001b[39m.\u001b[39mtext\n\u001b[1;32m    358\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:703\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    696\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    697\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    701\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    702\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 703\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:882\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m get_llm_cache() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    868\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    869\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    870\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    881\u001b[0m     ]\n\u001b[0;32m--> 882\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    883\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    884\u001b[0m     )\n\u001b[1;32m    885\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    886\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:740\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    739\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 740\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    741\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    742\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:727\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    718\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    719\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    724\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    725\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 727\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    728\u001b[0m                 prompts,\n\u001b[1;32m    729\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    730\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    731\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    732\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    733\u001b[0m             )\n\u001b[1;32m    734\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    735\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    736\u001b[0m         )\n\u001b[1;32m    737\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    738\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/ollama.py:413\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    412\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 413\u001b[0m     final_chunk \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_stream_with_aggregation(\n\u001b[1;32m    414\u001b[0m         prompt,\n\u001b[1;32m    415\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    416\u001b[0m         images\u001b[39m=\u001b[39;49mimages,\n\u001b[1;32m    417\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    418\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    419\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    421\u001b[0m     generations\u001b[39m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/ollama.py:329\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    322\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GenerationChunk:\n\u001b[1;32m    328\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mfor\u001b[39;49;00m stream_resp \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_generate_stream(prompt, stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    330\u001b[0m         \u001b[39mif\u001b[39;49;00m stream_resp:\n\u001b[1;32m    331\u001b[0m             chunk \u001b[39m=\u001b[39;49m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/ollama.py:176\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_generate_stream\u001b[39m(\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    170\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    174\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    175\u001b[0m     payload \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt, \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m: images}\n\u001b[0;32m--> 176\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_stream(\n\u001b[1;32m    177\u001b[0m         payload\u001b[39m=\u001b[39;49mpayload,\n\u001b[1;32m    178\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    179\u001b[0m         api_url\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_url\u001b[39m}\u001b[39;49;00m\u001b[39m/api/generate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    180\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/ollama.py:247\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[0;32m--> 247\u001b[0m         \u001b[39mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[1;32m    248\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mOllama call failed with status code 404. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMaybe your model is not found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand you should pull the model with `ollama pull \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m         )\n\u001b[1;32m    252\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         optional_detail \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=OllamaEmbeddings(model='phi3'),\n",
    "    collection_name=\"my_documents\",\n",
    "    path='local_qdrant'\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"phi3\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# llm = ChatOllama(base_url= 'http://localhost:11434', model=\"phi3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The name of the book is \"Motor neurone disease: the use of non-invasive ventilation in the management of motor neurone disease - NICE clinical guideline CG105.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is the name of the book?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 34 chunks\n"
     ]
    }
   ],
   "source": [
    "# Load web page\n",
    "import argparse\n",
    "\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embed and store\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings, OpenAIEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loader = Docx2txtLoader('../..//data/raw/podcast_transcript.docx')\n",
    "data = loader.load()\n",
    "\n",
    "# Split into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(f\"Split into {len(all_splits)} chunks\")\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits[:5], embedding=OllamaEmbeddings(model='phi3'))\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    all_splits,\n",
    "    OllamaEmbeddings(model='llama3'),\n",
    "    path=\"local_qdrant_podcast\",\n",
    "    collection_name=\"podcast2\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "qdrant = QdrantVectorStore.from_existing_collection(collection_name=\"podcast2\", embedding=OllamaEmbeddings( model='llama3'), path=\"local_qdrant_podcast\" )\n",
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", response_metadata={'model': 'llama3', 'created_at': '2024-07-28T19:46:41.66811425Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 6722614127, 'load_duration': 43087583, 'prompt_eval_count': 11, 'prompt_eval_duration': 2849224000, 'eval_count': 26, 'eval_duration': 3827461000}, id='run-b7920a1f-6d74-4d37-8d34-4271bead54de-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever = qdrant.as_retriever()\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0,\n",
    "    \n",
    "    # other params...\n",
    ")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm.invoke(\"hello\")\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Give me a title for the podcast episode you just received?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the retrieved context, it seems that the conversation is about data governance, quality, and analysis. Here are some key points:\\n\\n* Data stewards are responsible for ensuring data quality and improving it.\\n* The value derived from data is important to consider.\\n* SQL is necessary for data analysis, but not everyone needs to be an expert in it.\\n* The team has a standardized set of analyses that they run for all countries using Python scripts.\\n\\nSome potential questions that could be answered based on this context include:\\n\\n* What are the responsibilities of a data steward?\\n* How can data quality be improved?\\n* What is the value of data analysis in a business or organization?\\n\\nPlease let me know if you have any specific question you would like me to answer!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"from the podcast, create five key takeaway that i can add to a linkeedIn podcast.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I want to know the title \n",
    "I want to have a linkedin post \n",
    "i want to have a key takeaway for people who don't have time to watch the episode \n",
    "I want to summarize chapters based on time\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileIOLoader\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "failed to find libmagic.  Check your installation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m loader \u001b[39m=\u001b[39m UnstructuredFileIOLoader(io\u001b[39m.\u001b[39mBytesIO(file_content))\n\u001b[1;32m      4\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mlazy_load()\n\u001b[0;32m----> 5\u001b[0m doc \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(docs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:89\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlazy_load\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Document]:\n\u001b[1;32m     88\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:327\u001b[0m, in \u001b[0;36mUnstructuredFileIOLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m    325\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/unstructured/partition/auto.py:288\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, xml_keep_tags, data_source_metadata, metadata_filename, request_timeout, hi_res_model_name, model_name, date_from_file_object, starting_page_number, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m headers \u001b[39m!=\u001b[39m {}:\n\u001b[1;32m    284\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    285\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe headers kwarg is set but the url kwarg is not. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe headers kwarg will be ignored.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    287\u001b[0m         )\n\u001b[0;32m--> 288\u001b[0m     filetype \u001b[39m=\u001b[39m detect_filetype(\n\u001b[1;32m    289\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    290\u001b[0m         file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m    291\u001b[0m         file_filename\u001b[39m=\u001b[39;49mmetadata_filename,\n\u001b[1;32m    292\u001b[0m         content_type\u001b[39m=\u001b[39;49mcontent_type,\n\u001b[1;32m    293\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     file\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/unstructured/file_utils/filetype.py:77\u001b[0m, in \u001b[0;36mdetect_filetype\u001b[0;34m(filename, content_type, file, file_filename, encoding)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m# NOTE(robinson) - the python-magic docs recommend reading at least the first 2048 bytes\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# Increased to 4096 because otherwise .xlsx files get detected as a zip file\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# ref: https://github.com/ahupp/python-magic#usage\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m LIBMAGIC_AVAILABLE:\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmagic\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     mime_type \u001b[39m=\u001b[39m magic\u001b[39m.\u001b[39mfrom_buffer(file\u001b[39m.\u001b[39mread(\u001b[39m4096\u001b[39m), mime\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/magic/__init__.py:209\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m m\u001b[39m.\u001b[39mfrom_descriptor(fd)\n\u001b[1;32m    208\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m loader\n\u001b[0;32m--> 209\u001b[0m libmagic \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload_lib()\n\u001b[1;32m    211\u001b[0m magic_t \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_void_p\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrorcheck_null\u001b[39m(result, func, args):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/magic/loader.py:49\u001b[0m, in \u001b[0;36mload_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m   \u001b[39m# It is better to raise an ImportError since we are importing magic module\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mfailed to find libmagic.  Check your installation\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: failed to find libmagic.  Check your installation"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/raw/podcast_transcript.docx\", \"rb\") as file:\n",
    "    file_content = file.read()\n",
    "    loader = UnstructuredFileIOLoader(io.BytesIO(file_content))\n",
    "    docs = loader.lazy_load()\n",
    "    doc = next(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object UnstructuredBaseLoader.lazy_load at 0x31ad9d6d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
